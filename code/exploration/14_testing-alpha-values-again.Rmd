---
title: "Testing different values of alpha (again)"
author: "Eleanor Jackson"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  github_document:
    keep_html: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.path = "figures/14_testing-alpha-values-again/")
ggplot2::theme_set(ggplot2::theme_classic(base_size = 10))
```

I have re-run the models and done edge effect corrections and will now compare them.
When I compared models previously (in [12_testing-alpha-values.md](https://github.com/ee-jackson/spatial-premature-fruit-drop/blob/main/code/exploration/12_testing-alpha-values.md)) I hadn't yet done edge corrections.

Edge effect correction testing (like [this](https://github.com/ee-jackson/spatial-premature-fruit-drop/blob/main/code/exploration/03_edge-effects.md)) suggested that when: 

* alpha = 1/30, edge effect correction radius should be 90m
* alpha = 1/40, edge effect correction radius should be 110m
* alpha = 1/50, edge effect correction radius should be 120m
* alpha = 1/60, edge effect correction radius should be 150m

When alpha was 1/5, 1/10 or 1/20, tests suggested it was best *not* to correct for edge effects as the smaller radius wasn't very good for making predictions. So in these cases I removed traps from the dataset which were < the average migration distance away from the edge.

Because these models were fit with subset data I can't use LOO to compare them. 
But lets look at the other models.

## Compare the predictive accuracy of the models using LOO-CV

```{r message=FALSE}
model_05m <- readRDS(here::here("output", "models", "202307", "zoib_capsules_05m.rds"))
model_10m <- readRDS(here::here("output", "models", "202307","zoib_capsules_10m.rds"))
model_15m <- readRDS(here::here("output", "models", "202307","zoib_capsules_15m.rds"))
model_20m <- readRDS(here::here("output", "models", "202307","zoib_capsules_20m.rds"))
model_30m <- readRDS(here::here("output", "models", "202307","zoib_capsules_30m.rds"))
model_40m <- readRDS(here::here("output", "models", "202307","zoib_capsules_40m.rds"))
model_50m <- readRDS(here::here("output", "models", "202307","zoib_capsules_50m.rds"))
model_60m <- readRDS(here::here("output", "models", "202307", "zoib_capsules_60m.rds"))

library("tidyverse"); theme_set(theme_bw(base_size = 10))
library("broom.mixed")
library("brms")
library("ggdist")
library("loo") # v 2.4.1
library("patchwork")
options(mc.cores = 3)

```

## Compare the predictive accuracy of the models using Leave-One-Out Cross Validation

Leave-one-out cross-validation (LOO-CV) is a popular method for comparing Bayesian models based on their estimated predictive performance on new/unseen data.

Expected log predictive density (ELPD): If new observations are well-accounted by the posterior predictive distribution, then the density of the posterior predictive distribution is high and so is its logarithm. So higher ELPD = better predictive performance.

```{r loo-compare}
comp <- loo_compare(model_30m, model_40m, model_50m, model_60m)

print(comp, digits = 3)

comp %>% 
  data.frame() %>% 
  rownames_to_column(var = "model_name") %>% 
  ggplot(aes(x    = reorder(model_name, elpd_diff), 
             y    = elpd_diff, 
             ymin = elpd_diff - se_diff, 
             ymax = elpd_diff + se_diff)) +
  geom_pointrange(shape = 21, fill = "white") +
  coord_flip() +
  geom_hline(yintercept = 0, colour = "blue", linetype = 2) +
  labs(x = NULL, y = "difference from model with the largest ELPD", 
       title = "expected log predictive density (ELPD)") 


```

Of these, the 30m model looks the best.

Quick look at the models with smaller datasets.

```{r}
loo(model_05m)
loo(model_10m)
loo(model_15m)
loo(model_20m)
```

Of these, the 20m model looks the best. 
These results are similar to when I did this pre-edge effect corrections.

## Compare estimates

Has changing alpha changed the results of our model?

```{r estimate-compare, warning=FALSE}
tibble(model = c("model_05m","model_15m", "model_10m", "model_20m", "model_30m", 
                 "model_40m", "model_50m", "model_60m")) %>% 
  mutate(fit   = purrr::map(model, get)) %>% 
  mutate(tidy  = purrr::map(fit, tidy)) %>% 
  unnest(tidy) %>% 
  filter(effect == "fixed" & !grepl("(Intercept)", term)) -> my_coef_tab

my_coef_tab

my_coef_tab %>% 
  mutate(term = recode(term, connectivity_sc = "mu", 
                       phi_connectivity_sc = "phi",
                       zoi_connectivity_sc = "zoi", 
                       coi_connectivity_sc = "coi"
                       )) %>% 
  mutate(term = as.factor(term)) %>% 
  mutate(term = fct_relevel(term, c("mu", "phi", "zoi", "coi"))) %>%
  mutate(model = gsub("model_", "", model, fixed = TRUE)) %>% 
  ggplot(aes(x = model, y = estimate, ymin = conf.low, ymax = conf.high)) +
  geom_pointrange(shape = 21, fill = "white") +
  labs(x = "Average dispersal distance",
       y = "Estimate Â± CI [95%]") +
  geom_hline(yintercept = 0,  color = "blue") +
  coord_flip() +
  theme_classic() +
  facet_wrap(~term, ncol = 1)

```

Confidence intervals are all overlapping - no real change in parameter estimates between models.
Think I will probably go with the 30m model as it allows us to utilise more of the data and the decision doesnt seem to matter too much for the results.
